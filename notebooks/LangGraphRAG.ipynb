{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "\n",
    "urls = [\n",
    "        \"https://research.google/blog/\",\n",
    "        \"https://deepmind.google/discover/blog/\"\n",
    "    ]\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits, \n",
    "    collection_name=\"rag-chroma\", \n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about latest AI news from the web urls provided about Anthropic, Google Research and AWS.\"\n",
    ")\n",
    "\n",
    "rag_tool = [retriever_tool]\n",
    "retrieve = ToolNode([retriever_tool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind RAG tool to LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "llm_with_tools = llm.bind_tools(rag_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions for State Graph\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langchain_core.messages import  AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from typing import Literal\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Function to decide to call tool or not\n",
    "def decide_to_call_tool(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    \"\"\" Decide to call tool or not \"\"\"\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# Function to call model\n",
    "def call_model(state: MessagesState) -> MessagesState:\n",
    "    \"\"\" Call model \"\"\"\n",
    "    model_response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return state | {\"messages\": [model_response]}\n",
    "\n",
    "rag_graph = StateGraph(MessagesState)\n",
    "rag_graph.add_node(\"agent\", call_model)\n",
    "rag_graph.add_node(\"tools\", retrieve)\n",
    "rag_graph.add_edge(START, \"agent\")\n",
    "rag_graph.add_conditional_edges(\"agent\", decide_to_call_tool)\n",
    "rag_graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "rag_app = rag_graph.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke LLM by calling RAG app\n",
    "user_msg = input(\"User: \")\n",
    "messages = [HumanMessage(content=user_msg)]\n",
    "thread_id = {\"configurable\": {\"thread_id\":\"1\"}}\n",
    "ai_events = rag_app.invoke({\"messages\": messages},thread_id)\n",
    "\n",
    "for msg in ai_events[\"messages\"]:\n",
    "     msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
