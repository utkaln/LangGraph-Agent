{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import operator\n",
    "from langchain_core.messages import  AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "system_prompt = \"\"\" You are an event manager planning a seminar about various software engineering topics happening at Hunt Valley, Maryland. \n",
    "The user will ask you to inquire about topics of the seminar.\n",
    "The user will ask you to register for certain sessions, you need to ask them the required information and register for the topics.\n",
    "Always confirm with user before registering them for any topic.\n",
    "Allow the user to modify or cancel their registration.\n",
    "The user may inquire about their existing registrations, you need to provide them with the list of topics they are registered for.\n",
    "If certain topic that is not available as session, then note down the feedback from the user and thank them. But do not register for any topic that you are not provided with.\n",
    "Keep the chat limited to only the seminar topics and registration.\n",
    "If any of the tools are not available, then you need to inform the user about it and tell that this will be added in the next version.\n",
    "Do not engage into any emotional or intimate conversations with the user, politely decline if the user starts such a topic.\n",
    "At the end of a successful registration offer the user to book nearby hotels and suggest food options. For this you can use the TavilySearchResults tool.\n",
    "You can also provide information about tourist places and current weather using TavilySearchResults tool.\n",
    "\"\"\"\n",
    "\n",
    "welcome_prompt = \"\"\"Welcome to the Technology seminar of North Maryland Area. How can I assist you today? When you are done chatting, please write bye to end\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistrationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "memory = MemorySaver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM node and other nodes for the State Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistrationAgent:\n",
    "    def __init__(self,model,tools,checkpointer, system=\"\"):\n",
    "        # Save the system message as a class level attribute\n",
    "        self.system = system\n",
    "\n",
    "        # Initialize the state graph that will have one LLM node, One Tool node and one Action Edge\n",
    "        graph = StateGraph(RegistrationState)\n",
    "        # Start Node\n",
    "        graph.add_node(\"llm\",self.call_llm)\n",
    "        \n",
    "        # Action Node that is available as tool\n",
    "        graph.add_node(\"action\",self.action_node)\n",
    "\n",
    "        # Decision Edge to decide to use action node\n",
    "        # First parameter is the node from which this edge is coming from \n",
    "        # Second parameter is the function that let's langraph explore tools\n",
    "        # Third parameter is available nodes after the decision either action node or END node\n",
    "        graph.add_conditional_edges(\"llm\",self.action_edge_decision,{True: \"action\", False: END})\n",
    "\n",
    "        # Create Another edge to loop back to LLM node from action node\n",
    "        graph.add_edge(\"action\",\"llm\")\n",
    "\n",
    "        # Define what node the graph should start, in this case the llm\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        # Once setup done compile the graph and Save the graph at the class level\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "        # Create a dictionary of available tools sent to the constructor\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "        # Bind tools to model so that LLM can search for tools\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "    # Define function for call llm node\n",
    "    def call_llm(self, state: RegistrationState):\n",
    "        # get the messages saved in the Agent state object\n",
    "        messages = state[\"messages\"]\n",
    "        # If system message is not blank, append that to the beginning of the messages\n",
    "        if self.system:\n",
    "            system_message= [SystemMessage(content=self.system)]\n",
    "            messages =  system_message + messages\n",
    "        # Call the model with the messages, it should return response as a single message\n",
    "        resp = self.model.invoke(messages)\n",
    "        print(f\"Response from LLM -> {resp}\")\n",
    "        # Return the response message as a list, that will be appended to the existing messages due to operator.add annotation at class level\n",
    "        return {\"messages\": [resp]}\n",
    "    \n",
    "    # Define function for call action node\n",
    "    def action_node(self, state: RegistrationState):\n",
    "        # get the last message from the Agent State, since the last message is the response from LLM that suggests to use the tool\n",
    "        # tool calls attribute is expected which has the name of the tool to be called\n",
    "        referred_tools_list = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "\n",
    "        # Tool calls can be multiple tools, so iterate over them\n",
    "        for tool in referred_tools_list:\n",
    "            print(f\"Tool to be called -> {tool}\")\n",
    "            # invoke tool call by finding the name and the arguments as suggested by LLM\n",
    "            result = self.tools[tool[\"name\"]].invoke(tool[\"args\"])\n",
    "            # Append the result to the results list\n",
    "            results.append(ToolMessage(tool_call_id=tool[\"id\"], name=tool[\"name\"], content=str(result)))\n",
    "            \n",
    "        print(f\"Finished tool call ...\")\n",
    "        # returns results and add to the messages list at class level\n",
    "        return {\"messages\": results}\n",
    "    \n",
    "    # Define the actiton edge function that decides whether to look for tool or not\n",
    "    # If the last message in the message list has tool_calls attribute, then return True, else False\n",
    "    def action_edge_decision(self, state: RegistrationState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return  len(result.tool_calls) > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LLM with Agent Graph and Memory configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=3)\n",
    "reg_agent = RegistrationAgent(llm,[tool],memory, system_prompt)\n",
    "# Add a thread id to make the conversation persistent\n",
    "thread_id = {\"configurable\": {\"thread_id\":\"1\"}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Model\n",
    "- Apply recursive limit so that the chat will end after these many calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = [HumanMessage(content=input(\"User : \"))]\n",
    "events = reg_agent.graph.stream(\n",
    "    {\"messages\": user_input},\n",
    "    thread_id,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
