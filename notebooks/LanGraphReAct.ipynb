{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool name -> tavily_search_results_json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/hyz6y59163s8qzgrs9d1r7fw0000gn/T/ipykernel_25391/1418715925.py:11: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langgraph.graph import StateGraph, END \n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import  AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Define tool that is available to langraph that an action edge can find\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "print(f\"tool name -> {tool.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define A place holder for all the messages that is known as AgentState\n",
    "- This is a list of messages which keeps adding a new message everytime it is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Agent Class that performs:\n",
    "1. Call LLM in this example OpenAI\n",
    "2. Check if Action is present\n",
    "3. Take Action\n",
    "\n",
    "Steps in the code \n",
    "1. The constructor function takes model name, available tools choices and system prompt\n",
    "2. Start a LLM node\n",
    "3. Then add an action node\n",
    "4. Then define an action edge to link between LLM and action node. \n",
    "5. If no action decision made by LLM, then send to END node\n",
    "6. Create an edge to loop back to LLM node from Action Node\n",
    "7. Compile the graph and save it as class level attribute\n",
    "8. Create a dictionary of tools sent as parameters and save as class level attribute\n",
    "9. Save the tool name that sent as input as a class level attribute under the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,model,tools,system=\"\"):\n",
    "        # Save the system message as a class level attribute\n",
    "        self.system = system\n",
    "\n",
    "        # Initialize the state graph that will have one LLM node, One Tool node and one Action Edge\n",
    "        graph = StateGraph(AgentState)\n",
    "        # Start Node\n",
    "        graph.add_node(\"llm\",self.call_openai)\n",
    "        \n",
    "        # Action Node that is available as tool\n",
    "        graph.add_node(\"action\",self.action_node)\n",
    "\n",
    "        # Decision Edge to decide to use action node\n",
    "        # First parameter is the node from which this edge is coming from \n",
    "        # Second parameter is the function that let's langraph explore tools\n",
    "        # Third parameter is available nodes after the decision either action node or END node\n",
    "        graph.add_conditional_edges(\"llm\",self.action_edge,{True: \"action\", False: END})\n",
    "\n",
    "        # Create Another edge to loop back to LLM node from action node\n",
    "        graph.add_edge(\"action\",\"llm\")\n",
    "\n",
    "        # Define what node the graph should start, in this case the llm\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        # Once setup done compile the graph and Save the graph at the class level\n",
    "        self.graph = graph.compile()\n",
    "\n",
    "        # Create a dictionary of available tools sent to the constructor\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "        # Bind tools to model so that LLM can search for tools\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "    # Define function for call llm node\n",
    "    def call_openai(self, state: AgentState):\n",
    "        # get the messages saved in the Agent state object\n",
    "        messages = state[\"messages\"]\n",
    "        # If system message is not blank, append that to the beginning of the messages\n",
    "        if self.system:\n",
    "            system_message= [SystemMessage(content=self.system)]\n",
    "            messages =  system_message + messages\n",
    "        # Call the model with the messages, it should return response as a single message\n",
    "        resp = self.model.invoke(messages)\n",
    "        print(f\"Response from LLM -> {resp}\")\n",
    "        # Return the response message as a list, that will be appended to the existing messages due to operator.add annotation at class level\n",
    "        return {\"messages\": [resp]}\n",
    "    \n",
    "    # Define function for call action node\n",
    "    def action_node(self, state: AgentState):\n",
    "        # get the last message from the Agent State, since the last message is the response from LLM that suggests to use the tool\n",
    "        # tool calls attribute is expected which has the name of the tool to be called\n",
    "        referred_tools_list = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "\n",
    "        # Tool calls can be multiple tools, so iterate over them\n",
    "        for tool in referred_tools_list:\n",
    "            print(f\"Tool to be called -> {tool}\")\n",
    "            # invoke tool call by finding the name and the arguments as suggested by LLM\n",
    "            result = self.tools[tool[\"name\"]].invoke(tool[\"args\"])\n",
    "            # Append the result to the results list\n",
    "            results.append(ToolMessage(tool_call_id=tool[\"id\"], name=tool[\"name\"], content=str(result)))\n",
    "            \n",
    "        print(f\"Finished tool call ...\")\n",
    "        # returns results and add to the messages list at class level\n",
    "        return {\"messages\": results}\n",
    "    \n",
    "    # Define the actiton edge function that decides whether to look for tool or not\n",
    "    # If the last message in the message list has tool_calls attribute, then return True, else False\n",
    "    def action_edge(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return  len(result.tool_calls) > 0\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a chat model with system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "ai_agent = Agent(model,[tool],system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the langraph with user message as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is the capital of France?\"\n",
    "messages = [HumanMessage(content=user_prompt)]\n",
    "result = ai_agent.graph.invoke({\"messages\": messages})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
